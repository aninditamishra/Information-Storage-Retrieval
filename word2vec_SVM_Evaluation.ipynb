{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSCE 670 :: Information Storage and Retrieval :: Texas A&M University :: Spring 2019\n",
    "\n",
    "\n",
    "# Homework 2:   word2vec + SVM + Evaluation\n",
    "\n",
    "### 100 points [6% of your final grade]\n",
    "\n",
    "### Due: Tuesday, February 26, 2019 by 11:59pm\n",
    "\n",
    "*Goals of this homework:* Understand word2vec-like term embeddings,  explore real-world challenges with SVM-based classifiers, understand and implement several evaluation metrics.\n",
    "\n",
    "*Submission instructions (eCampus):* To submit your homework, rename this notebook as `UIN_hw2.ipynb`. For example, my homework submission would be something like `555001234_hw2.ipynb`. Submit this notebook via eCampus (look for the homework 2 assignment there). Your notebook should be completely self-contained, with the results visible in the notebook. We should not have to run any code from the command line, nor should we have to run your code within the notebook (though we reserve the right to do so). So please run all the cells for us, and then submit.\n",
    "\n",
    "*Late submission policy:* For this homework, you may use as many late days as you like (up to the 5 total allotted to you).\n",
    "\n",
    "*Collaboration policy:* You are expected to complete each homework independently. Your solution should be written by you without the direct aid or help of anyone else. However, we believe that collaboration and team work are important for facilitating learning, so we encourage you to discuss problems and general problem approaches (but not actual solutions) with your classmates. You may post on Piazza, search StackOverflow, etc. But if you do get help in this way, you must inform us by **filling out the Collaboration Declarations at the bottom of this notebook**. \n",
    "\n",
    "*Example: I found helpful code on stackoverflow at https://stackoverflow.com/questions/11764539/writing-fizzbuzz that helped me solve Problem 2.*\n",
    "\n",
    "The basic rule is that no student should explicitly share a solution with another student (and thereby circumvent the basic learning process), but it is okay to share general approaches, directions, and so on. If you feel like you have an issue that needs clarification, feel free to contact either me or the TA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Term embeddings + SVM (80 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "\n",
    "For this homework, we will still play with Yelp reviews from the [Yelp Dataset Challenge](https://www.yelp.com/dataset_challenge). As in Homework 1, you'll see that each line corresponds to a review on a particular business. Each review has a unique \"ID\" and the text content is in the \"review\" field. Additionally, this time, we also offer you the \"label\". If `label=1`, it means that this review is `Food-relevant`. If `label=0`, it means that this review is `Food-irrelevant`. Similarly, we have already done some basic preprocessing on the reviews, so you can just tokenize each review using whitespace.\n",
    "\n",
    "There are about 40,000 reviews in total, in which about 20,000 reviews are \"Food-irrelevant\". We split the review data into two sets. *review_train.json* is the training set. *review_test.json* is the testing set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please load the dataset\n",
    "\n",
    "import json\n",
    "import zipfile\n",
    "\n",
    "reviewsTest = []\n",
    "reviewsTrain = []\n",
    "\n",
    "zip_ref = zipfile.ZipFile(\"hw2_data.zip\", 'r')\n",
    "zip_ref.extractall()\n",
    "zip_ref.close()\n",
    "\n",
    "for line in open('hw2_data/review_test.json', 'r'):\n",
    "    reviewsTest.append(json.loads(line))\n",
    "    \n",
    "for line in open('hw2_data/review_train.json', 'r'):\n",
    "    reviewsTrain.append(json.loads(line))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Pre-trained term embeddings\n",
    "\n",
    "To save your time, you can make use of  pre-trained term embeddings. In this homework, we are using one of the great pre-trained models from [GloVe](https://nlp.stanford.edu/projects/glove/) based on 2 billion tweets. GloVe is quite similar to word2vec. Unzip the *glove.6B.50d.txt.zip* file and run the code below. You will be able to load the term embeddings model, with which each word can be represented with a 50-dimension vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# reload the pre-trained term embeddings\n",
    "import numpy as np\n",
    "\n",
    "zip_ref = zipfile.ZipFile(\"hw2_data/glove.6B.50d.txt.zip\", 'r')\n",
    "zip_ref.extractall()\n",
    "zip_ref.close()\n",
    "\n",
    "with open(\"glove.6B.50d.txt\", \"rb\") as lines:\n",
    "    model = {line.split()[0]: np.array( list( map(float, line.split( )[1:]) ))\n",
    "           for line in lines} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you have a vector representation for each word. First, we use the simple (arithmetic) **mean** of these vectors of words in a review to represent the review. *Note: Just ignore those words which are not in the corpus of this pre-trained model.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Please figure out the vector representation for each review in the training data and testing data.\n",
    "testReviewVector  =[]\n",
    "trainReviewVector =[]\n",
    "\n",
    "def makeReviewVector(review):\n",
    "    x=[]\n",
    "    x=review.split()\n",
    "    sumReviewVector = 0\n",
    "    countWords = 0\n",
    "    for word in x:\n",
    "        if word.encode() in model:\n",
    "            encodedWord = word.encode()\n",
    "            sumReviewVector+= model[encodedWord]\n",
    "        countWords+=1\n",
    "    return (sumReviewVector/countWords)\n",
    "\n",
    "for i in range(0,len(reviewsTest)):\n",
    "    vect = makeReviewVector(reviewsTest[i]['review'])\n",
    "    testReviewVector.append({'id': reviewsTest[i]['id'], 'review': vect,'label':reviewsTest[i]['label']})\n",
    "\n",
    "for i in range(0,len(reviewsTrain)):\n",
    "    vect = makeReviewVector(reviewsTrain[i]['review'])\n",
    "    trainReviewVector.append({'id': reviewsTrain[i]['id'], 'review': vect,'label':reviewsTrain[i]['label']})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM\n",
    "\n",
    "With the vector representations you get for each review, please train an SVM model to predict whether a given review is food-relevant or not. **You do not need to implement any classifier from scratch. You may use scikit-learn's built-in capabilities.** You can only train your model with reviews in *review_train.json*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SVM model training\n",
    "from sklearn import svm\n",
    "\n",
    "xTrain =[]\n",
    "xTest =[]\n",
    "yTrain =[]\n",
    "yTest=[]\n",
    "\n",
    "for i in range(0,len(trainReviewVector)):\n",
    "    xTrain.append(trainReviewVector[i]['review'])\n",
    "    yTrain.append(trainReviewVector[i]['label'])\n",
    "\n",
    "for i in range(0,len(testReviewVector)):\n",
    "    xTest.append(testReviewVector[i]['review'])\n",
    "    yTest.append(testReviewVector[i]['label'])\n",
    "    \n",
    "clf = svm.SVC(kernel='linear')\n",
    "clf.fit(xTrain, yTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "yPredict = clf.predict(xTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your goal is to predict whether a given review is food-relevant or not. Please report the overall accuracy, precision and recall of your model on the **testing data**. You should **implement the functions for accuracy, precision, and recall**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp:   5487\n",
      "fp:   650\n",
      "tn:   5325\n",
      "fn:   458\n",
      "\n",
      "accuracy:   90.70469798657717\n",
      "precision:   89.4085057845853\n",
      "recall:   92.29604709840203\n",
      "F1 Score:   90.82933289190531\n"
     ]
    }
   ],
   "source": [
    "tp = 0 #true positive\n",
    "fp = 0 #false positive\n",
    "tn = 0 #true negative\n",
    "fn = 0  #false negative\n",
    "\n",
    "for i in range(0,len(yTest)):\n",
    "    if(yTest[i] == 1 and  yPredict[i]== 1):\n",
    "        tp+=1\n",
    "    elif(yTest[i] ==0 and  yPredict[i] ==1):\n",
    "        fp+=1\n",
    "    elif(yTest[i] ==0 and  yPredict[i] ==0):\n",
    "        tn+=1\n",
    "    elif(yTest[i] ==1 and  yPredict[i] ==0):\n",
    "        fn+=1\n",
    "        \n",
    "print(\"tp:  \",tp)\n",
    "print(\"fp:  \",fp)\n",
    "print(\"tn:  \",tn)\n",
    "print(\"fn:  \",fn)\n",
    "\n",
    "\n",
    "accuracy = (tp + tn)/len(yTest)\n",
    "precision = tp/(tp+fp)\n",
    "recall = tp/(tp+fn)\n",
    "f1Score = 2*((precision*recall)/(precision+recall))\n",
    "print()\n",
    "print(\"accuracy:  \",accuracy*100)\n",
    "print(\"precision:  \",precision*100)\n",
    "print(\"recall:  \",recall*100)\n",
    "print(\"F1 Score:  \",f1Score*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document-based embeddings\n",
    "\n",
    "Instead of taking the mean of term embeddings, you can directly train a **doc2vec** model for paragraph or document embeddings. You can refer to the paper [Distributed Representations of Sentences and Documents](https://arxiv.org/pdf/1405.4053v2.pdf) for more details. And in this homework, you can make use of the implementation in [gensim](https://radimrehurek.com/gensim/models/doc2vec.html).\n",
    "\n",
    "Now, you need to:\n",
    "* Train a doc2vec model based on all reviews you have (training + testing sets).\n",
    "* Use the embeddings from your doc2vec model to represent each review and train a new SVM model.\n",
    "* Report the overall accuracy, precision and recall of your model on the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a doc2vec\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.test.utils import get_tmpfile\n",
    "\n",
    "allReviews = []\n",
    "\n",
    "for i in range(0,len(reviewsTrain)):\n",
    "    allReviews.append(reviewsTrain[i]['review'].split())\n",
    "\n",
    "for i in range(0,len(reviewsTest)):\n",
    "    allReviews.append(reviewsTest[i]['review'].split())\n",
    " \n",
    "\n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(allReviews)]\n",
    "model = Doc2Vec(documents, vector_size=50, window=2, min_count=1, workers=4)\n",
    "\n",
    "\n",
    "fname = get_tmpfile(\"my_doc2vec_model\")\n",
    "model.save(fname)\n",
    "model = Doc2Vec.load(fname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anindita/anaconda3/lib/python3.7/site-packages/gensim/models/doc2vec.py:580: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anindita/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:17: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1\n",
      "iteration 2\n",
      "iteration 3\n",
      "iteration 4\n",
      "iteration 5\n",
      "iteration 6\n",
      "iteration 7\n",
      "iteration 8\n",
      "iteration 9\n",
      "iteration 10\n",
      "iteration 11\n",
      "iteration 12\n",
      "iteration 13\n",
      "iteration 14\n",
      "iteration 15\n",
      "iteration 16\n",
      "iteration 17\n",
      "iteration 18\n",
      "iteration 19\n",
      "iteration 20\n",
      "iteration 21\n",
      "iteration 22\n",
      "iteration 23\n",
      "iteration 24\n",
      "iteration 25\n",
      "iteration 26\n",
      "iteration 27\n",
      "iteration 28\n",
      "iteration 29\n",
      "iteration 30\n",
      "iteration 31\n",
      "iteration 32\n",
      "iteration 33\n",
      "iteration 34\n",
      "iteration 35\n",
      "iteration 36\n",
      "iteration 37\n",
      "iteration 38\n",
      "iteration 39\n",
      "Model Saved\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 40\n",
    "vec_size = 20\n",
    "alpha = 0.025\n",
    "\n",
    "new_model = Doc2Vec(size=vec_size,\n",
    "                alpha=alpha, \n",
    "                min_alpha=0.00025,\n",
    "                min_count=1,\n",
    "                dm =1)\n",
    "  \n",
    "new_model.build_vocab(documents)\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    print('iteration {0}'.format(epoch))\n",
    "    new_model.train(documents,\n",
    "                total_examples=new_model.corpus_count,\n",
    "                epochs=new_model.iter)\n",
    "    # decrease the learning rate\n",
    "    new_model.alpha -= 0.0002\n",
    "    # fix the learning rate, no decay\n",
    "    new_model.min_alpha = model.alpha\n",
    "\n",
    "new_model.save(\"d2v.model\")\n",
    "print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a SVM\n",
    "xTrain = []\n",
    "xTest = []\n",
    "\n",
    "for i in range(0,len(reviewsTrain)):\n",
    "    xTrain.append(new_model.infer_vector(reviewsTrain[i]['review'].split()))\n",
    "    \n",
    "for i in range(0,len(reviewsTest)):\n",
    "    xTest.append(new_model.infer_vector(reviewsTest[i]['review'].split()))\n",
    "\n",
    "clf = svm.SVC(kernel='linear')\n",
    "clf.fit(xTrain, yTrain)\n",
    "yPredict = clf.predict(xTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp:   5469\n",
      "fp:   606\n",
      "tn:   5369\n",
      "fn:   476\n",
      "\n",
      "accuracy:   90.92281879194631\n",
      "precision:   90.02469135802468\n",
      "recall:   91.9932716568545\n",
      "F1 Score:   90.99833610648919\n"
     ]
    }
   ],
   "source": [
    "# Report the performance\n",
    "tp = 0 #true positive\n",
    "fp = 0 #false positive\n",
    "tn = 0 #true negative\n",
    "fn = 0  #false negative\n",
    "\n",
    "for i in range(0,len(yTest)):\n",
    "    if(yTest[i] == 1 and  yPredict[i]== 1):\n",
    "        tp+=1\n",
    "    elif(yTest[i] ==0 and  yPredict[i] ==1):\n",
    "        fp+=1\n",
    "    elif(yTest[i] ==0 and  yPredict[i] ==0):\n",
    "        tn+=1\n",
    "    elif(yTest[i] ==1 and  yPredict[i] ==0):\n",
    "        fn+=1\n",
    "        \n",
    "print(\"tp:  \",tp)\n",
    "print(\"fp:  \",fp)\n",
    "print(\"tn:  \",tn)\n",
    "print(\"fn:  \",fn)\n",
    "\n",
    "\n",
    "accuracy = (tp + tn)/len(yTest)\n",
    "precision = tp/(tp+fp)\n",
    "recall = tp/(tp+fn)\n",
    "f1Score = 2*((precision*recall)/(precision+recall))\n",
    "\n",
    "print()\n",
    "print(\"accuracy:  \", accuracy*100)\n",
    "print(\"precision:  \", precision*100)\n",
    "print(\"recall:  \",recall*100)\n",
    "print(\"F1 Score:  \",f1Score*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you observe? How different are your results for the term-based average approach vs. the doc2vec approach? Why do you think this is?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values for accuracy and F1 scores for both my approaches (Glove (term based approach) and doc2vec approach) are quite close. The doc2vec (trained over 40 epochs) model has performed only slightly better than my term-based approach.\n",
    "\n",
    "*Results:*\n",
    "1. For term based approach(vector size:50): <br>\n",
    "    a. Accuracy:  90.704   \n",
    "    b. F1 Score:  90.704 <br>\n",
    "    \n",
    "2. For doc2vec (vector size:20) trained over 40 epochs: <br>\n",
    "    a. Accuracy:  90.704    \n",
    "    b. F1 Score:  90.704 <br>\n",
    "   \n",
    "\n",
    "*Reasoning:*\n",
    "Each Doc2Vec vector captures the semantic meaning of all the words in the context. But the same can be achieved with term based embeddings as well since each word in Glove preserves its own semantic meaning. Summing up all the vectors and averaging them will result in a vector which could have all the semantics preserved. For example, when we add the vectors (transport+water) the result nearly equals ship or boat, which means summing the vectors sums up the semantics. \n",
    "\n",
    "Summing up all the individual word vectors and averaging them to represent each document as vector isn't very computationally strenuos because our document lengths are relatively small. Therefore, both approaches are equally good.\n",
    "\n",
    "However, it's debatable whether both of these approaches should be compared directly because:\n",
    "1. The Glove model has been trained over 6 billion words whereas the Doc2Vec model was trained exclusively with a vocabulary containing words all the available reviews (test + train). \n",
    "2. The Glove model couldn't handle \"out of vocabulary\" words. In such cases, I had to assume the word vector to be 0. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can you do better?\n",
    "\n",
    "Finally, see if you can do better than either the word- or doc- based embeddings approach for classification. You may explore new features, new classifiers, etc. Whatever you like. Just provide your code and a justification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import numpy as np\n",
    "def load_vectors(fname):\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        data[tokens[0]] = np.array(list(map(float, tokens[1:])))\n",
    "    return data\n",
    "\n",
    "vect_fastText= load_vectors(\"crawl-300d-2M-subword/crawl-300d-2M-subword.vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "tp:   5684\n",
      "fp:   546\n",
      "tn:   5429\n",
      "fn:   261\n",
      "\n",
      "accuracy:   93.22986577181209\n",
      "precision:   91.23595505617978\n",
      "recall:   95.60975609756098\n",
      "F1 Score:   93.37166324435317\n"
     ]
    }
   ],
   "source": [
    "testReviewVector  =[]\n",
    "trainReviewVector =[]\n",
    "\n",
    "def makeReviewVector(review):\n",
    "    x=[]\n",
    "    x=review.split()\n",
    "    sumReviewVector = 0\n",
    "    countWords = 0\n",
    "    for word in x:\n",
    "        if word in vect_fastText:\n",
    "            sumReviewVector+= vect_fastText[word]\n",
    "        countWords+=1\n",
    "    return (sumReviewVector/countWords)\n",
    "\n",
    "for i in range(0,len(reviewsTest)):\n",
    "    vect = makeReviewVector(reviewsTest[i]['review'])\n",
    "    testReviewVector.append({'id': reviewsTest[i]['id'], 'review': vect,'label':reviewsTest[i]['label']})\n",
    "\n",
    "for i in range(0,len(reviewsTrain)):\n",
    "    vect = makeReviewVector(reviewsTrain[i]['review'])\n",
    "    trainReviewVector.append({'id': reviewsTrain[i]['id'], 'review': vect,'label':reviewsTrain[i]['label']})\n",
    "    \n",
    "    \n",
    "from sklearn import svm\n",
    "xTrain =[]\n",
    "xTest =[]\n",
    "yTrain =[]\n",
    "yTest=[]\n",
    "\n",
    "for i in range(0,len(trainReviewVector)):\n",
    "    xTrain.append(trainReviewVector[i]['review'])\n",
    "    yTrain.append(trainReviewVector[i]['label'])\n",
    "\n",
    "for i in range(0,len(testReviewVector)):\n",
    "    xTest.append(testReviewVector[i]['review'])\n",
    "    yTest.append(testReviewVector[i]['label'])\n",
    "    \n",
    "clf = svm.SVC(kernel='linear')\n",
    "clf.fit(xTrain, yTrain)\n",
    "\n",
    "yPredict = clf.predict(xTest)\n",
    "\n",
    "tp = 0 #true positive\n",
    "fp = 0 #false positive\n",
    "tn = 0 #true negative\n",
    "fn = 0  #false negative\n",
    "\n",
    "for i in range(0,len(yTest)):\n",
    "    if(yTest[i] == 1 and  yPredict[i]== 1):\n",
    "        tp+=1\n",
    "    elif(yTest[i] ==0 and  yPredict[i] ==1):\n",
    "        fp+=1\n",
    "    elif(yTest[i] ==0 and  yPredict[i] ==0):\n",
    "        tn+=1\n",
    "    elif(yTest[i] ==1 and  yPredict[i] ==0):\n",
    "        fn+=1\n",
    "        \n",
    "print(\"tp:  \",tp)\n",
    "print(\"fp:  \",fp)\n",
    "print(\"tn:  \",tn)\n",
    "print(\"fn:  \",fn)\n",
    "\n",
    "\n",
    "accuracy = (tp + tn)/len(yTest)\n",
    "precision = tp/(tp+fp)\n",
    "recall = tp/(tp+fn)\n",
    "f1Score = 2*((precision*recall)/(precision+recall))\n",
    "\n",
    "print()\n",
    "print(\"accuracy:  \", accuracy*100)\n",
    "print(\"precision:  \", precision*100)\n",
    "print(\"recall:  \",recall*100)\n",
    "print(\"F1 Score:  \",f1Score*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Justification:*\n",
    "\n",
    "Glove and Doc2Vec learn vectors for complete words in the training corpus whereas FastText learns vectors for character n-grams that are found within each word as well as each complete word. The word vector that we get in FastText already contains embedded sub-word information. \n",
    "This is advantageous because:\n",
    "1. Character n-grams are capable of handling out of vocabulary words. The vector for a rare word can be formed from its character n-grams even if word doesn't appear in training corpus. \n",
    "2. Rare words have better word embeddings because their character n-grams are still shared with other words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: NDCG (20 points)\n",
    "\n",
    "You calculated the recall and precision in Part 1 and now you get a chance to implement NDCG. \n",
    "\n",
    "Assume that Amy searches for \"food-relevant\" reviews in the **testing set** on two search engines `A` and `B`. Since the ground-truth labels for the reviews are unknown to A and B, they need to make a prediction for each review and then return a ranked list of results based on their probabilities. The results from A are in *search_result_A.json*, and the results from B are in *search_result_B.json*. Each line contains the id of a review and its corresponding ranking.\n",
    "\n",
    "You can check their labels in *review_test.json* while calculating the NDCG scores. If a review is \"food-relevant\", the relevance score is 1. Otherwise, the relevance score is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "searchResultA = []\n",
    "searchResultB = []\n",
    "\n",
    "for line in open('hw2_data/search_result_A.json', 'r'):\n",
    "    searchResultA.append(json.loads(line))\n",
    "    \n",
    "for line in open('hw2_data/search_result_B.json', 'r'):\n",
    "    searchResultB.append(json.loads(line))\n",
    "     \n",
    "relevanceTest = {}\n",
    "\n",
    "for i in range(0,len(reviewsTest)):\n",
    "    relevanceTest[reviewsTest[i]['id']] = reviewsTest[i]['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NDCG for search_result_A.json\n",
    "\n",
    "import math\n",
    "from operator import itemgetter \n",
    "\n",
    "dcg_A = 0\n",
    "\n",
    "for i in range(0,len(searchResultA)): \n",
    "    num = 0\n",
    "    den = 0\n",
    "    num = (pow(2,relevanceTest[searchResultA[i]['id']]) - 1)\n",
    "    den = math.log2(i+2)\n",
    "    dcg_A += (num/den)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NDCG for search_result_B.json\n",
    "\n",
    "dcg_B = 0\n",
    "\n",
    "for i in range(0,len(searchResultB)): \n",
    "    num = 0\n",
    "    den = 0\n",
    "    num = (pow(2,relevanceTest[searchResultB[i]['id']]) - 1)\n",
    "    den = math.log2(i+2)\n",
    "    dcg_B+= (num/den)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG for search engine A: 0.9182854705835336\n",
      "NDCG for search engine B: 0.9866215613328989\n"
     ]
    }
   ],
   "source": [
    "relevanceReviewTest = {}\n",
    "#monotonically decreasing sort of all known relevance judgments\n",
    "relevanceReviewTest = sorted(relevanceTest.items(), key = itemgetter(1), reverse = True)\n",
    "\n",
    "lengthSearchA = len(searchResultA) #to find relevant results for search engine A\n",
    "lengthSearchB = len(searchResultB) #to find relevant results for search engine B \n",
    "\n",
    "idcg_a = 0\n",
    "idcg_b = 0\n",
    "\n",
    "i1 = 1\n",
    "i2 = 1\n",
    "\n",
    "for key,value in relevanceReviewTest: \n",
    "    if i1<lengthSearchA:\n",
    "        num = 0\n",
    "        den = 0\n",
    "        num = (pow(2,value) - 1)\n",
    "        den = math.log2(i1+1)\n",
    "        i1+=1\n",
    "        idcg_a+= (num/den)\n",
    "    if i2<lengthSearchB:\n",
    "        num = 0\n",
    "        den = 0\n",
    "        num = (pow(2,value) - 1)\n",
    "        den = math.log2(i2+1)\n",
    "        i2+=1\n",
    "        idcg_b+= (num/den)\n",
    "    \n",
    "ndcg_a = dcg_A/idcg_a\n",
    "ndcg_b = dcg_B/idcg_b\n",
    "\n",
    "\n",
    "print(\"NDCG for search engine A:\",ndcg_a)\n",
    "print(\"NDCG for search engine B:\",ndcg_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaboration declarations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*If you collaborated with anyone (see Collaboration policy at the top of this homework), you can put your collaboration declarations here.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " I found helpful code on Medium at https://medium.com/@mishra.thedeepak/doc2vec-simple-implementation-example-df2afbbfbad5 for doc2vec"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
