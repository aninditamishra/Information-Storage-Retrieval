{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSCE 670 :: Information Storage and Retrieval :: Texas A&M University :: Spring 2019\n",
    "\n",
    "\n",
    "# Homework 3:   Collaborative Filtering + Matrix Factorization\n",
    "\n",
    "### 100 points [6% of your final grade]\n",
    "\n",
    "### Due: Wednesday, March 27, 2019 by 11:59pm\n",
    "\n",
    "*Goals of this homework:* Understand how collaborative filtering and matrix factorization works. Explore different methods for real-world recommendation senarios.\n",
    "\n",
    "*Submission instructions (eCampus):* To submit your homework, rename this notebook as `UIN_hw3.ipynb`. For example, my homework submission would be something like `555001234_hw3.ipynb`. Submit this notebook via eCampus (look for the homework 3 assignment there). Your notebook should be completely self-contained, with the results visible in the notebook. We should not have to run any code from the command line, nor should we have to run your code within the notebook (though we reserve the right to do so). So please run all the cells for us, and then submit.\n",
    "\n",
    "*Late submission policy:* For this homework, you may use as many late days as you like (up to the 5 total allotted to you).\n",
    "\n",
    "*Collaboration policy:* You are expected to complete each homework independently. Your solution should be written by you without the direct aid or help of anyone else. However, we believe that collaboration and team work are important for facilitating learning, so we encourage you to discuss problems and general problem approaches (but not actual solutions) with your classmates. You may post on Piazza, search StackOverflow, etc. But if you do get help in this way, you must inform us by **filling out the Collaboration Declarations at the bottom of this notebook**. \n",
    "\n",
    "*Example: I found helpful code on stackoverflow at https://stackoverflow.com/questions/11764539/writing-fizzbuzz that helped me solve Problem 2.*\n",
    "\n",
    "The basic rule is that no student should explicitly share a solution with another student (and thereby circumvent the basic learning process), but it is okay to share general approaches, directions, and so on. If you feel like you have an issue that needs clarification, feel free to contact either me or the TA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1. Collaborative Filtering (50 points)\n",
    "\n",
    "In this part, you will implement collaborative filtering on the [MovieLens Latest Datasets](http://files.grouplens.org/datasets/movielens/ml-latest-small-README.html). After removing users who left less than 20 ratings and movies with less than 20 ratings, the provided dataset has only ~1,200 items and ~500 users. You can also check the title and genres of each movie in *movies_info.csv*.\n",
    "\n",
    "As background, read [Collaborative Filtering Recommender Systems](http://files.grouplens.org/papers/FnT%20CF%20Recsys%20Survey.pdf) and refer to the course slides `week06rec.pdf` for collaborative filtering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Load the Data\n",
    "\n",
    "Please download the dataset from Piazza. There are about 65,000 ratings in total. We split the rating data into two set. You will train with 70% of the data (in *train_movie.csv*) and test on the remaining 30% of data (in *test_movie.csv*). Each of train and test files has lines having this format: UserID, MovieID, Rating. \n",
    "\n",
    "First you will need to load the data and store it with any structure you like. Please report the numbers of unique users and movies in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique users: 541\n",
      "Number of unique movie: 1211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anindita/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:11: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/Users/anindita/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:12: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "# load the data, then print out the number of\n",
    "# movies and users in each of train and test sets.\n",
    "import pandas as pd\n",
    "\n",
    "trainFile = pd.read_csv('train_movie.csv')\n",
    "testFile = pd.read_csv('test_movie.csv') \n",
    "\n",
    "trainFile.columns = [\"userID\",\"movieID\",\"rating\"]\n",
    "testFile.columns = [\"userID\",\"movieID\",\"rating\"]\n",
    "\n",
    "trainMatrix = trainFile.pivot(index=\"userID\", columns=\"movieID\", values=\"rating\").as_matrix()\n",
    "testMatrix = testFile.pivot(index=\"userID\", columns=\"movieID\", values=\"rating\").as_matrix()\n",
    "\n",
    "print(\"Number of unique users:\", trainMatrix.shape[0])\n",
    "print(\"Number of unique movie:\", trainMatrix.shape[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Implement User-based Collaborative Filtering\n",
    "\n",
    "In this part, you will implement the basic Userâ€“User Collaborative Filtering algorithm introduced in the class. Given the ratings by each user, you are going to try different methods in calculating the similarities between users. You will use equation `(c)` in `week06rec.pdf` (Page 40) to aggregate ratings. Set k = 0.05. Just consider all users as neighbors. That is, while predicting how user $u$ will rate item $i$, $\\widehat{C}$ includes all users who have ratings on i in the training set.\n",
    "\n",
    "*For this memory-based algorithm, you can only rely on the ratings in training set to predict for the testing set. That is, you assume that you don't know any ratings information in the test set except that when you evalaute your model.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity\n",
    "\n",
    "First, you will try to calculate the similarity between users with cosine similary following the equation on page 16 of [Collaborative Filtering Recommender Systems](http://files.grouplens.org/papers/FnT%20CF%20Recsys%20Survey.pdf). And then you need to predict the rating for each (user, movie) tuple in the test set. *Note: you don't need to subtract user mean baseline from the ratings prior to computing the similarity.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "#cosine similarity function\n",
    "def cos_sim(a, b):\n",
    "    dot_product=0\n",
    "    norm_a=0\n",
    "    norm_b=0\n",
    "    for i in range(0,len(a)):\n",
    "        if not math.isnan(a[i]) and not math.isnan(b[i]):\n",
    "            dot_product += a[i]*b[i]\n",
    "        if not math.isnan(a[i]):\n",
    "            norm_a+=math.pow(a[i],2)\n",
    "        if not math.isnan(b[i]):\n",
    "            norm_b+=math.pow(b[i],2)\n",
    "    return (dot_product/ (math.sqrt(norm_a) * math.sqrt(norm_b)))\n",
    "\n",
    "trainCosine=np.zeros((len(trainMatrix),len(trainMatrix)))\n",
    "\n",
    "#construct cosine similarity matrix\n",
    "for user in range(0,len(trainMatrix)):\n",
    "    for u in range(0,len(trainMatrix)):\n",
    "        trainCosine[user][u]= cos_sim(trainMatrix[user],trainMatrix[u])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "userMean = []\n",
    "\n",
    "#Compute mean user ratings\n",
    "for u in range(0,trainMatrix.shape[0]):\n",
    "    sum=0\n",
    "    count=0\n",
    "    for i in range(0,trainMatrix.shape[1]):\n",
    "        if not math.isnan(trainMatrix[u][i]):\n",
    "            sum+=trainMatrix[u][i]\n",
    "            count+=1\n",
    "    userMean.append(sum/count)\n",
    "\n",
    "k = 0.05\n",
    "predictedRatings = np.zeros((trainMatrix.shape[0],trainMatrix.shape[1]))\n",
    "\n",
    "for user in range(0,trainMatrix.shape[0]):\n",
    "    rMean = userMean[user]\n",
    "    for item in range(0,trainMatrix.shape[1]):\n",
    "        val = 0\n",
    "        for s in range(0,trainMatrix.shape[0]):\n",
    "            if(trainCosine[user][s]!=1):\n",
    "                if not math.isnan(trainMatrix[s][item]):\n",
    "                    val += trainCosine[user][s] * ( trainMatrix[s][item]- userMean[s])\n",
    "        predictedRatings[user][item]= rMean + k*val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to calculate Root Mean Square Error\n",
    "def rmse():\n",
    "    rmse = 0\n",
    "    count = 0\n",
    "    squareSum =0\n",
    "    for user in range(0,testMatrix.shape[0]):\n",
    "        for item in range(0,testMatrix.shape[1]):\n",
    "            if not math.isnan(testMatrix[user][item]):\n",
    "                temp=0\n",
    "                temp = testMatrix[user][item] - predictedRatings[user][item]\n",
    "                squareSum+= math.pow(temp,2)\n",
    "                count+=1\n",
    "    rmse = math.sqrt(squareSum/count)\n",
    "    return rmse\n",
    "\n",
    "\n",
    "#Function to calculate Mean Absolute Error\n",
    "def mae():\n",
    "    mae = 0\n",
    "    count = 0\n",
    "    sum =0\n",
    "    for user in range(0,testMatrix.shape[0]):\n",
    "        for item in range(0,testMatrix.shape[1]):\n",
    "            if not math.isnan(testMatrix[user][item]):\n",
    "                temp=0\n",
    "                temp = abs(testMatrix[user][item] - predictedRatings[user][item])\n",
    "                sum+=temp\n",
    "                count+=1\n",
    "    mae = (sum/count)\n",
    "    return mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "You should evaluate your predictions using Mean Absolute Error and Root Mean Squared Error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error 0.8991489107420851\n",
      "Mean Absolute Error: 0.6903937198616394\n"
     ]
    }
   ],
   "source": [
    "# Report Mean Absolute Error and Root Mean Squared Error for test set\n",
    "print(\"Root Mean Squared Error\",rmse())   \n",
    "print(\"Mean Absolute Error:\",mae())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pearson correlation\n",
    "\n",
    "Then, you will try to calculate the similarity between users with pearson correlation following `week06rec.pdf` (Page 37). And then you need to predict the rating for each (user, movie) tuple in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pearson correlation function\n",
    "def pearson_correlation(a, b):\n",
    "    num = 0\n",
    "    den1 = 0\n",
    "    den2 = 0\n",
    "    res = 0\n",
    "    for i in range(0,trainMatrix.shape[1]):\n",
    "        if not math.isnan(trainMatrix[a][i]):\n",
    "            if not math.isnan(trainMatrix[b][i]):\n",
    "                num_a = (trainMatrix[a][i] - userMean[a])\n",
    "                num_b = (trainMatrix[b][i] - userMean[b])\n",
    "                num += num_a * num_b\n",
    "                den1 += math.pow(num_a,2)\n",
    "                den2 += math.pow(num_b,2)\n",
    "    if(den1==0) or (den2==0):\n",
    "        return 0\n",
    "    res = num/(math.sqrt(den1*den2))\n",
    "    return res\n",
    "\n",
    "trainPearson=np.zeros((len(trainMatrix),len(trainMatrix)))\n",
    "\n",
    "#construct pearson correlation matrix\n",
    "for user in range(0,trainMatrix.shape[0]):\n",
    "    for u in range(0,trainMatrix.shape[0]):\n",
    "        trainPearson[user][u]= pearson_correlation(user,u)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 0.05\n",
    "predictedRatings = np.zeros((trainMatrix.shape[0],trainMatrix.shape[1]))\n",
    "\n",
    "for user in range(0,trainMatrix.shape[0]):\n",
    "    rMean = userMean[user]\n",
    "    for item in range(0,trainMatrix.shape[1]):\n",
    "        val = 0\n",
    "        for s in range(0,trainMatrix.shape[0]):\n",
    "                if not math.isnan(trainMatrix[s][item]):\n",
    "                    val += trainPearson[user][s] * ( trainMatrix[s][item]- userMean[s])\n",
    "        predictedRatings[user][item]= rMean + k*val\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "You should evaluate your predictions using Mean Absolute Error and Root Mean Squared Error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error 0.898338623508709\n",
      "Mean Absolute Error: 0.6917506759527964\n"
     ]
    }
   ],
   "source": [
    "# Report Mean Absolute Error and Root Mean Squared Error for test\n",
    "print(\"Root Mean Squared Error\",rmse())   \n",
    "print(\"Mean Absolute Error:\",mae())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pearson correlation (varying the threshold)\n",
    "\n",
    "In [Collaborative Filtering Recommender Systems](http://files.grouplens.org/papers/FnT%20CF%20Recsys%20Survey.pdf), they observe that: \n",
    "\n",
    "> Pearson correlation suffers from computing high similarity\n",
    "between users with few ratings in common. This can be alleviated by setting a threshold on the number of co-rated items\n",
    "necessary for full agreement (correlation of 1) and scaling the\n",
    "similarity when the number of co-rated items falls below this\n",
    "threshold.\n",
    "\n",
    "So now revise your Pearson to consider a threshold. Try several values and report for one that you think is appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anindita/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "k = 0.05\n",
    "\n",
    "def pearson_correlation_w_threshold_Util(a, b, threshold):\n",
    "    num = 0\n",
    "    den1 = 0\n",
    "    den2 = 0\n",
    "    res = 0\n",
    "    intersecting_elements = 0\n",
    "    for i in range(0,trainMatrix.shape[1]):\n",
    "        if not math.isnan(trainMatrix[a][i]):\n",
    "            if not math.isnan(trainMatrix[b][i]):\n",
    "                intersecting_elements +=1\n",
    "                num_a = (trainMatrix[a][i] - userMean[a])\n",
    "                num_b = (trainMatrix[b][i] - userMean[b])\n",
    "                num += num_a * num_b\n",
    "                den1 += math.pow(num_a,2)\n",
    "                den2 += math.pow(num_b,2)\n",
    "    if(den1==0) or (den2==0):\n",
    "        return 0\n",
    "    res = (num/(math.sqrt(den1*den2))) \n",
    "    return (res* min(intersecting_elements/threshold,1))\n",
    "\n",
    "#construct pearson correlation matrix\n",
    "def pearson_correlation_w_threshold(threshold):\n",
    "    trainPearson=np.zeros((len(trainMatrix),len(trainMatrix)))\n",
    "    for user in range(0,trainMatrix.shape[0]):\n",
    "        for u in range(0,trainMatrix.shape[0]):\n",
    "            trainPearson[user][u]= pearson_correlation_w_threshold_Util(user,u,threshold)\n",
    "    return trainPearson\n",
    "\n",
    "def predictRatings(trainPearson):\n",
    "    predictedRatings = np.zeros((trainMatrix.shape[0],trainMatrix.shape[1]))\n",
    "    for user in range(0,trainMatrix.shape[0]):\n",
    "        rMean = userMean[user]\n",
    "        for item in range(0,trainMatrix.shape[1]):\n",
    "            val = 0\n",
    "            for s in range(0,trainMatrix.shape[0]):\n",
    "                    if not math.isnan(trainMatrix[s][item]):\n",
    "                        val += trainPearson[user][s] * ( trainMatrix[s][item]- userMean[s])\n",
    "            predictedRatings[user][item]= rMean + k*val\n",
    "    return predictedRatings\n",
    "\n",
    "trainPearson_10=np.zeros((len(trainMatrix),len(trainMatrix)))\n",
    "trainPearson_15=np.zeros((len(trainMatrix),len(trainMatrix)))\n",
    "trainPearson_20=np.zeros((len(trainMatrix),len(trainMatrix)))\n",
    "trainPearson_25=np.zeros((len(trainMatrix),len(trainMatrix)))\n",
    "trainPearson_30=np.zeros((len(trainMatrix),len(trainMatrix)))\n",
    "trainPearson_50=np.zeros((len(trainMatrix),len(trainMatrix)))\n",
    "\n",
    "trainPearson_10 = pearson_correlation_w_threshold(10)\n",
    "trainPearson_15 = pearson_correlation_w_threshold(15)\n",
    "trainPearson_20 = pearson_correlation_w_threshold(20)\n",
    "trainPearson_25 = pearson_correlation_w_threshold(25)\n",
    "trainPearson_30 = pearson_correlation_w_threshold(30)\n",
    "trainPearson_50 = pearson_correlation_w_threshold(50)\n",
    "\n",
    "predictedRatings_10 = np.zeros((trainMatrix.shape[0],trainMatrix.shape[1]))\n",
    "predictedRatings_15 = np.zeros((trainMatrix.shape[0],trainMatrix.shape[1]))\n",
    "predictedRatings_20 = np.zeros((trainMatrix.shape[0],trainMatrix.shape[1]))\n",
    "predictedRatings_25 = np.zeros((trainMatrix.shape[0],trainMatrix.shape[1]))\n",
    "predictedRatings_30 = np.zeros((trainMatrix.shape[0],trainMatrix.shape[1]))\n",
    "predictedRatings_50 = np.zeros((trainMatrix.shape[0],trainMatrix.shape[1]))\n",
    "\n",
    "predictedRatings_10 = predictRatings(trainPearson_10)\n",
    "predictedRatings_15 = predictRatings(trainPearson_15)\n",
    "predictedRatings_20 = predictRatings(trainPearson_20)\n",
    "predictedRatings_25 = predictRatings(trainPearson_25)\n",
    "predictedRatings_30 = predictRatings(trainPearson_30)\n",
    "predictedRatings_50 = predictRatings(trainPearson_50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(predictedRatings):\n",
    "    rmse = 0\n",
    "    count = 0\n",
    "    squareSum =0\n",
    "    for user in range(0,testMatrix.shape[0]):\n",
    "        for item in range(0,testMatrix.shape[1]):\n",
    "            if not math.isnan(testMatrix[user][item]):\n",
    "                temp=0\n",
    "                temp = testMatrix[user][item] - predictedRatings[user][item]\n",
    "                squareSum+= math.pow(temp,2)\n",
    "                count+=1\n",
    "    rmse = math.sqrt(squareSum/count)\n",
    "    return rmse\n",
    "\n",
    "\n",
    "#Function to calculate Mean Absolute Error\n",
    "def mae(predictedRatings):\n",
    "    mae = 0\n",
    "    count = 0\n",
    "    sum =0\n",
    "    for user in range(0,testMatrix.shape[0]):\n",
    "        for item in range(0,testMatrix.shape[1]):\n",
    "            if not math.isnan(testMatrix[user][item]):\n",
    "                temp=0\n",
    "                temp = abs(testMatrix[user][item] - predictedRatings[user][item])\n",
    "                sum+=temp\n",
    "                count+=1\n",
    "    mae = (sum/count)\n",
    "    return mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "You should evaluate your predictions using Mean Absolute Error and Root Mean Squared Error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold 10\n",
      "Root Mean Squared Error 0.890270143854433\n",
      "Mean Absolute Error: 0.6851235437901917\n",
      "\n",
      "Threshold 15\n",
      "Root Mean Squared Error 0.8905005640168924\n",
      "Mean Absolute Error: 0.6853868941141172\n",
      "\n",
      "Threshold 20\n",
      "Root Mean Squared Error 0.891857899121167\n",
      "Mean Absolute Error: 0.6866426283525587\n",
      "\n",
      "Threshold 25\n",
      "Root Mean Squared Error 0.8936574572009665\n",
      "Mean Absolute Error: 0.6883253384684949\n",
      "\n",
      "Threshold 30\n",
      "Root Mean Squared Error 0.8956008686671971\n",
      "Mean Absolute Error: 0.6901420890207776\n",
      "\n",
      "Threshold 50\n",
      "Root Mean Squared Error 0.9026670391381861\n",
      "Mean Absolute Error: 0.6969055995369728\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Report Mean Absolute Error and Root Mean Squared Error for test\n",
    "t = 10 \n",
    "thresholds = [predictedRatings_10, predictedRatings_15, predictedRatings_20, predictedRatings_25, predictedRatings_30, predictedRatings_50]\n",
    "for threshold in thresholds:\n",
    "    print(\"Threshold\", t )\n",
    "    print(\"Root Mean Squared Error\",rmse(threshold))   \n",
    "    print(\"Mean Absolute Error:\",mae(threshold))\n",
    "    if(t==30):\n",
    "        t+=20\n",
    "    else:\n",
    "        t+=5\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you observe? How different are your results for the original Pearson Correlation approach vs. the thresholded version vs. the Cosine Similarity approach? Why do you think this is? *provide a brief (1-2 paragraph) discussion based on these questions.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cosine Similarity** \n",
    "\n",
    "RMSE- 0.8991<br>\n",
    "MAE- 0.6903<br>\n",
    "\n",
    "**Pearson Correlation**\n",
    "\n",
    "RMSE- 0.8983<br>\n",
    "MAE- 0.6917<br>\n",
    "\n",
    "**Pearson Correlation with Scaling**\n",
    "\n",
    "a) **Threshold = 10**<br>\n",
    "\n",
    "   RMSE-0.8902<br>\n",
    "   MAE- 0.6851<br>\n",
    "   \n",
    "b) **Threshold = 25**<br>\n",
    "\n",
    "   RMSE-0.8936<br>\n",
    "   MAE- 0.6883<br>\n",
    "   \n",
    "c) **Threshold = 50**<br>\n",
    "\n",
    "   RMSE-0.902<br>\n",
    "   MAE- 0.696<br>\n",
    "   \n",
    "**Pearson correlation with scaling (with threshold = 10) performed the best out of the three approaches.**\n",
    "\n",
    "**This is because:**\n",
    "\n",
    "1)Cosine Similarity considers unknown ratings as '0' which implies that the user hated the movie. If the user hasn't watched the movie or just hasn't rated the movie then this might be a misrepresentation. \n",
    "\n",
    "2)Pearson Correlation computes statistical correlation between 2 users common ratings to determine their similarity. However, it suffers from computing high similarity between users with few ratings in common.\n",
    "\n",
    "3)Pearson correlation with scaling limits the neighborhood size which can result in more accurate predictions. Neighbors with low correlation introduce more noise than signal into the process. A threshold value of 10 (amongst 10, 15, 10, 25, 30 and 50) worked best with our dataset (541 users * 1211 movies). The RMSE increased as the threshold value was increased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2. MF (20 points)\n",
    "\n",
    "In class, we introduced how matrix factorization works for recommendation. Now it is your term to implement it. There are different methods to obtain the latent factor matrices **P** and **Q**, like gradient descent, Alternating Least Squares (ALS), and so on. Pick one of them and implement your MF model. *You can refer to tutorials and resources online. Remember our **collaboration policy** and you need to inform us of the resources you refer to.* \n",
    "\n",
    "Please report MAE and RMSE of your MF model for the test set. *You are expected to get a lower MAE and RMSE compared with the results in Part 1.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def matrix_factorization(R, P, Q, K, steps, alpha, beta):\n",
    "    Q = Q.T\n",
    "    for step in range(steps):\n",
    "        for i in range(len(R)):\n",
    "            for j in range(len(R[i])):\n",
    "                if not math.isnan(R[i][j]):\n",
    "                    eij = R[i][j] - numpy.dot(P[i,:],Q[:,j])\n",
    "                    for k in range(K):\n",
    "                        P[i][k] = P[i][k] + alpha * (2 * eij * Q[k][j] - beta * P[i][k])\n",
    "                        Q[k][j] = Q[k][j] + alpha * (2 * eij * P[i][k] - beta * Q[k][j]) \n",
    "    return numpy.dot(P, Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = trainMatrix.shape[0]\n",
    "M = trainMatrix.shape[1]\n",
    "K=3\n",
    "P = numpy.random.normal(scale= float(1/K), size = (N,K))\n",
    "Q = numpy.random.normal(scale = float(1/K), size = (M,K))\n",
    "\n",
    "def rmse(predictedRatings):\n",
    "    rmse = 0\n",
    "    count = 0\n",
    "    squareSum =0\n",
    "    for user in range(0,testMatrix.shape[0]):\n",
    "        for item in range(0,testMatrix.shape[1]):\n",
    "            if not math.isnan(testMatrix[user][item]):\n",
    "                temp=0\n",
    "                temp = testMatrix[user][item] - predictedRatings[user][item]\n",
    "                squareSum+= math.pow(temp,2)\n",
    "                count+=1\n",
    "    rmse = math.sqrt(squareSum/count)\n",
    "    return rmse\n",
    "\n",
    "\n",
    "#Function to calculate Mean Absolute Error\n",
    "def mae(predictedRatings):\n",
    "    mae = 0\n",
    "    count = 0\n",
    "    sum =0\n",
    "    for user in range(0,testMatrix.shape[0]):\n",
    "        for item in range(0,testMatrix.shape[1]):\n",
    "            if not math.isnan(testMatrix[user][item]):\n",
    "                temp=0\n",
    "                temp = abs(testMatrix[user][item] - predictedRatings[user][item])\n",
    "                sum+=temp\n",
    "                count+=1\n",
    "    mae = (sum/count)\n",
    "    return mae\n",
    "\n",
    "predictedRatings = np.zeros((N,M))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[nan, nan, nan, ..., nan, nan, nan],\n",
       "       [nan, nan, nan, ..., nan, nan, nan],\n",
       "       [nan, nan, nan, ..., nan, nan, nan],\n",
       "       [nan, nan, nan, ..., nan, nan, nan],\n",
       "       [nan, nan, nan, ..., nan, nan, nan],\n",
       "       [nan,  4., nan, ..., nan, nan, nan]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainMatrix[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error 0.8985096127672352\n",
      "Mean Absolute Error: 0.6893739842834072\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "predictedRatings = matrix_factorization(trainMatrix,P,Q,K,steps=100, alpha=0.001, beta=0.00145)\n",
    "print(\"Root Mean Squared Error\",rmse(predictedRatings))   \n",
    "print(\"Mean Absolute Error:\",mae(predictedRatings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.67011613, 4.0713135 , 3.62738925, ..., 4.57329504, 4.87069363,\n",
       "        5.02430444],\n",
       "       [3.43558993, 3.09701278, 2.87123875, ..., 3.36376298, 3.18105007,\n",
       "        3.57922887],\n",
       "       [1.12025432, 1.09521857, 1.01367472, ..., 1.44665716, 1.06270388,\n",
       "        1.35254493],\n",
       "       ...,\n",
       "       [3.03908535, 2.78232532, 2.26772728, ..., 4.33025683, 4.04614443,\n",
       "        4.21311288],\n",
       "       [3.7061566 , 3.23923871, 3.0203154 , ..., 3.15437003, 3.34167245,\n",
       "        3.59359754],\n",
       "       [3.85930549, 3.44278386, 3.09356335, ..., 4.0057682 , 3.95073561,\n",
       "        4.24572912]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictedRatings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error 0.893970464473602\n",
      "Mean Absolute Error: 0.6861664277554949\n"
     ]
    }
   ],
   "source": [
    "predictedRatings = matrix_factorization(trainMatrix,P,Q,K,steps=100, alpha=0.001, beta=0.0015)\n",
    "print(\"Root Mean Squared Error\",rmse(predictedRatings))   \n",
    "print(\"Mean Absolute Error:\",mae(predictedRatings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error 0.8881835092200071\n",
      "Mean Absolute Error: 0.680724006299843\n"
     ]
    }
   ],
   "source": [
    "K=2\n",
    "P = numpy.random.normal(scale= float(1/K), size = (N,K))\n",
    "Q = numpy.random.normal(scale = float(1/K), size = (M,K))\n",
    "predictedRatings = matrix_factorization(trainMatrix,P,Q,K,steps=100, alpha=0.001, beta=0.0015)\n",
    "print(\"Root Mean Squared Error\",rmse(predictedRatings))   \n",
    "print(\"Mean Absolute Error:\",mae(predictedRatings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error 0.8975586386308012\n",
      "Mean Absolute Error: 0.6877768422182007\n"
     ]
    }
   ],
   "source": [
    "K=7\n",
    "P = numpy.random.normal(scale= float(1/K), size = (N,K))\n",
    "Q = numpy.random.normal(scale = float(1/K), size = (M,K))\n",
    "predictedRatings = matrix_factorization(trainMatrix,P,Q,K,steps=100, alpha=0.001, beta=0.0045)\n",
    "print(\"Root Mean Squared Error\",rmse(predictedRatings))   \n",
    "print(\"Mean Absolute Error:\",mae(predictedRatings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error 0.8930243455546183\n",
      "Mean Absolute Error: 0.682734397973006\n"
     ]
    }
   ],
   "source": [
    "K=4\n",
    "P = numpy.random.normal(scale= float(1/K), size = (N,K))\n",
    "Q = numpy.random.normal(scale = float(1/K), size = (M,K))\n",
    "predictedRatings = matrix_factorization(trainMatrix,P,Q,K,steps=100, alpha=0.001, beta=0.00125)\n",
    "print(\"Root Mean Squared Error\",rmse(predictedRatings))   \n",
    "print(\"Mean Absolute Error:\",mae(predictedRatings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error 0.9029429322486798\n",
      "Mean Absolute Error: 0.6921217263259118\n"
     ]
    }
   ],
   "source": [
    "K=4\n",
    "P = numpy.random.normal(scale= float(1/K), size = (N,K))\n",
    "Q = numpy.random.normal(scale = float(1/K), size = (M,K))\n",
    "predictedRatings = matrix_factorization(trainMatrix,P,Q,K,steps=100, alpha=0.001, beta=0.0015)\n",
    "print(\"Root Mean Squared Error\",rmse(predictedRatings))   \n",
    "print(\"Mean Absolute Error:\",mae(predictedRatings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error 0.8906820785559076\n",
      "Mean Absolute Error: 0.6846433369958664\n"
     ]
    }
   ],
   "source": [
    "K=4\n",
    "P = numpy.random.normal(scale= float(1/K), size = (N,K))\n",
    "Q = numpy.random.normal(scale = float(1/K), size = (M,K))\n",
    "predictedRatings = matrix_factorization(trainMatrix,P,Q,K,steps=100, alpha=0.001, beta=0.0015)\n",
    "print(\"Root Mean Squared Error\",rmse(predictedRatings))   \n",
    "print(\"Mean Absolute Error:\",mae(predictedRatings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error 0.9007205331227398\n",
      "Mean Absolute Error: 0.6882902908325119\n"
     ]
    }
   ],
   "source": [
    "K=3\n",
    "P = numpy.random.normal(scale= float(1/K), size = (N,K))\n",
    "Q = numpy.random.normal(scale = float(1/K), size = (M,K))\n",
    "predictedRatings = matrix_factorization(trainMatrix,P,Q,K,steps=100, alpha=0.001, beta=0.0015)\n",
    "print(\"Root Mean Squared Error\",rmse(predictedRatings))   \n",
    "print(\"Mean Absolute Error:\",mae(predictedRatings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which method did you use to obtain **P** and **Q**? What are the advantages and disadvantages of the method you pick? *provide a brief (1-2 paragraph) discussion based on these questions.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used **Stochastic Gradient Descent (SGD)** to obtain latent representations of users and movies.<br>\n",
    "\n",
    "**Advantages** of SGD over Alternating Least Squares (ALS):<br>\n",
    "1. SGD is easier to implement and is efficient.\n",
    "2. SGD iteration is faster compared to ALS because its not as computationally intensive.\n",
    "\n",
    "**Disadvantages** of SGD:<br>\n",
    "1. As compared to ALS, SGD needs more iterations to obtain the same level of accuracy.\n",
    "2. Its performance is sensitive to the choice of the learning rate. \n",
    "3. Unlike ALS, parallelization of SGD is challenging.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3. Extension (30 points)\n",
    "\n",
    "Given your results in the previous parts, can you do better? For this last part you should report on your best attempt at improving MAE and RMSE. Provide code, results, plus a brief discussion on your approach. Hints: You may consider using the title or genres information, trying other algorithms, designing a hybrid system, and so on. *As in the last homework, you can do anything you like to improve MAE and RMSE.*\n",
    "\n",
    "You will get full marks for this part if you get better results than both of your CF and MF (of course we will also judge whether what you do here is reasonable or not). Additionally, you will get 5 points as bonus if your model performs the best among the whole class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Item-item CF\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "#cosine similarity function\n",
    "def cos_sim(a, b):\n",
    "    dot_product=0\n",
    "    norm_a=0.0\n",
    "    norm_b=0.0\n",
    "    for i in range(0,len(a)):\n",
    "        if not math.isnan(a[i]) and not math.isnan(b[i]):\n",
    "            dot_product += a[i]*b[i]\n",
    "        if not math.isnan(a[i]):\n",
    "            norm_a+=math.pow(a[i],2)\n",
    "        if not math.isnan(b[i]):\n",
    "            norm_b+=math.pow(b[i],2)\n",
    "    return (dot_product/ (math.sqrt(norm_a) * math.sqrt(norm_b)))\n",
    "\n",
    "#construct cosine similarity matrix\n",
    "if(trainMatrix.shape[0]==541):\n",
    "    trainMatrix = trainMatrix.T #item x user\n",
    "    \n",
    "trainCosine=np.zeros((len(trainMatrix),len(trainMatrix)))\n",
    "\n",
    "for item in range(0,len(trainMatrix)):\n",
    "    for i in range(0,len(trainMatrix)):\n",
    "        trainCosine[item][i]= cos_sim(trainMatrix[item],trainMatrix[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "movies =[]\n",
    "with open('movies_info.csv') as fin:\n",
    "    rows = (line.split() for line in fin)\n",
    "    for row in rows:\n",
    "        movies.append(row)\n",
    "\n",
    "movieGenres = []\n",
    "\n",
    "for movie in range (1,len(movies)):\n",
    "    movieGenres.append(movies[movie][-1])\n",
    "    \n",
    "#set of Genres that we have \n",
    "Genres  = [] \n",
    "for genre in movieGenres:\n",
    "    x=genre.split('|')\n",
    "    for i in x:\n",
    "        Genres.append(i)\n",
    "\n",
    "sGenres = set(Genres)\n",
    "\n",
    "#split the string for genres to get individual genre\n",
    "for i in range (0, len(movieGenres)):\n",
    "    movieGenres[i] = movieGenres[i].split('|')\n",
    "    \n",
    "vecGenres = []\n",
    "for m1 in movieGenres:\n",
    "    temp = {'Action':0, 'Adventure':0,'Animation':0, 'Children':0,'Comedy':0, 'Crime':0, 'Documentary':0, 'Drama':0,'Fantasy':0,'Film-Noir':0,'Horror':0,'IMAX':0,'Musical':0,'Mystery':0,'Romance':0, 'Sci-Fi':0,'Thriller':0,'War':0,'Western':0 }\n",
    "    for x in range(0,len(m1)):\n",
    "            temp[m1[x]] = 1\n",
    "    vecGenres.append(temp)\n",
    "    \n",
    "def cos_sim_content(a, b):\n",
    "    dot_product=0\n",
    "    norm_a=0.0\n",
    "    norm_b=0.0\n",
    "    for genre in sGenres:\n",
    "        dot_product += a[genre]*b[genre]\n",
    "        norm_a+=math.pow(a[genre],2)\n",
    "        norm_b+=math.pow(b[genre],2)\n",
    "    return (dot_product/ (math.sqrt(norm_a) * math.sqrt(norm_b)))\n",
    "\n",
    "cosineMatrix = np.zeros((len(movieGenres),len(movieGenres)))\n",
    "\n",
    "for m in range(0,len(vecGenres)):\n",
    "    for n in range(0,len(vecGenres)):\n",
    "        cosineMatrix[m][n]= cos_sim_content(vecGenres[m],vecGenres[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.5527275 , 0.27350603, ..., 0.08417788, 0.30855061,\n",
       "        0.05711708],\n",
       "       [0.5527275 , 1.        , 0.08620977, ..., 0.08627652, 0.39101573,\n",
       "        0.12642038],\n",
       "       [0.27350603, 0.08620977, 1.        , ..., 0.0265849 , 0.0048825 ,\n",
       "        0.02831792],\n",
       "       ...,\n",
       "       [0.08417788, 0.08627652, 0.0265849 , ..., 1.        , 0.44122603,\n",
       "        0.56838204],\n",
       "       [0.30855061, 0.39101573, 0.0048825 , ..., 0.44122603, 1.        ,\n",
       "        0.60343348],\n",
       "       [0.05711708, 0.12642038, 0.02831792, ..., 0.56838204, 0.60343348,\n",
       "        1.        ]])"
      ]
     },
     "execution_count": 513,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Take average of both the cosine matrices that I have. \n",
    "# cosineMatrix - Content based using movie genres\n",
    "# trainCosine - Item-Item CF for user*item matrix\n",
    "avg_cosScore=(trainCosine+cosineMatrix)/2\n",
    "avg_cosScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [],
   "source": [
    "itemMean = []\n",
    "#Compute mean item ratings\n",
    "for i in range(0,trainMatrix.shape[0]):\n",
    "    sum=0\n",
    "    count=0\n",
    "    for u in range(0,trainMatrix.shape[1]):\n",
    "        if not math.isnan(trainMatrix[i][u]):\n",
    "            sum+=trainMatrix[i][u]\n",
    "            count+=1\n",
    "    itemMean.append(sum/count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(trainMatrix.shape)\n",
    "predictedRatings = np.zeros((trainMatrix.shape[0],trainMatrix.shape[1]))\n",
    "\n",
    "for item in range(0,trainMatrix.shape[0]):\n",
    "    iMean = itemMean[item]\n",
    "    for user in range(0,trainMatrix.shape[1]):\n",
    "        val = 0\n",
    "        tot_cos = 0\n",
    "        for s in range(0,trainMatrix.shape[0]):\n",
    "            if(trainCosine[item][s]!=1):\n",
    "                if not math.isnan(trainMatrix[s][user]):\n",
    "                    val += avg_cosScore[item][s] * (trainMatrix[s][user]- itemMean[s])\n",
    "                    tot_cos += abs(avg_cosScore[item][s])\n",
    "        predictedRatings[item][user]= iMean + float(val/tot_cos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.84263715, 3.74933971, 1.49196837, ..., 3.52114138, 3.74995444,\n",
       "        4.12560946],\n",
       "       [4.29428234, 3.04341855, 1.51836043, ..., 3.00615714, 3.10030957,\n",
       "        3.56645185],\n",
       "       [4.12277257, 3.4028694 , 0.22452412, ..., 2.76307598, 3.0103708 ,\n",
       "        3.46175284],\n",
       "       ...,\n",
       "       [4.65313344, 3.32868516, 3.29195196, ..., 3.7972527 , 3.62217315,\n",
       "        4.12316432],\n",
       "       [4.68972163, 3.52263614, 3.17016203, ..., 3.67237632, 3.60565048,\n",
       "        4.04937533],\n",
       "       [5.00293062, 3.91622908, 3.57282585, ..., 4.15057594, 4.04216046,\n",
       "        4.41069337]])"
      ]
     },
     "execution_count": 516,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictedRatings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error 0.8689594285741693\n",
      "Mean Absolute Error: 0.6681990167907204\n"
     ]
    }
   ],
   "source": [
    "if(testMatrix.shape[0] == 541):\n",
    "    testMatrix = testMatrix.T\n",
    "\n",
    "def rmse():\n",
    "    rmse = 0\n",
    "    count = 0\n",
    "    squareSum =0\n",
    "    for user in range(0,testMatrix.shape[0]):\n",
    "        for item in range(0,testMatrix.shape[1]):\n",
    "            if not math.isnan(testMatrix[user][item]):\n",
    "                temp=0\n",
    "                temp = testMatrix[user][item] - predictedRatings[user][item]\n",
    "                squareSum+= math.pow(temp,2)\n",
    "                count+=1\n",
    "    rmse = math.sqrt(squareSum/count)\n",
    "    return rmse\n",
    "\n",
    "\n",
    "#Function to calculate Mean Absolute Error\n",
    "def mae():\n",
    "    mae = 0\n",
    "    count = 0\n",
    "    sum =0\n",
    "    for user in range(0,testMatrix.shape[0]):\n",
    "        for item in range(0,testMatrix.shape[1]):\n",
    "            if not math.isnan(testMatrix[user][item]):\n",
    "                temp=0\n",
    "                temp = abs(testMatrix[user][item] - predictedRatings[user][item])\n",
    "                sum+=temp\n",
    "                count+=1\n",
    "    mae = (sum/count)\n",
    "    return mae\n",
    "\n",
    "print(\"Root Mean Squared Error\",rmse())   \n",
    "print(\"Mean Absolute Error:\",mae())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please explain what you do to improve the recommendation in 1-2 paragraphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until now, we used user-user collaborative filtering which suffers from:\n",
    "1. Data Sparsity: In case of large number of items, number of items a user has rated reduces to a tiny percentage making the correlation coefficient less reliable\n",
    "2. User profiles change quickly and the entire system model had to be recomputed which is both time and computationally expensive\n",
    "\n",
    "So, I decided to use item-item collaborative filtering. This method is quite stable in itself as compared to User based collaborative filtering because the average item has a lot more ratings than the average user. So an individual rating doesnâ€™t impact as much. However, popularity of a movie can also easily influence collaborative recommendations. Therfore, introducing a content based filtering approach over metadata (such as movie genre) could be an useful feature to consider.\n",
    "\n",
    "I simply averaged the cosine similarities to ensemble the results from the content and the collaborative models and then used a simple weighted average approach to predict ratings. I was able to achieve an RMSE of 0.8690 and MAE of 0.6682. \n",
    "\n",
    "By ensembling item-item CF and content-based results I was able to outperform user-user CF (with cosine similarity, pearson correlation and pearson correlation with scaling approaches). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaboration declarations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*If you collaborated with anyone (see Collaboration policy at the top of this homework), you can put your collaboration declarations here.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "For MF, I found helpful suggestions on http://www.quuxlabs.com/blog/2010/09/matrix-factorization-a-simple-tutorial-and-implementation-in-python/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
